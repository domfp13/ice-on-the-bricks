{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Compute Management: The Hidden Complexity Tax\n",
    "\n",
    "**Purpose:** Demonstrate operational overhead of Databricks cluster management vs Snowflake warehouses  \n",
    "**Key Finding:** Engineers spend **X+ hours/week** managing Databricks clusters, with **40-60% cost overruns** from misconfiguration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Executive Summary\n",
    "\n",
    "Databricks compute management requires expertise across:\n",
    "- Driver node selection (6+ instance families)\n",
    "- Worker node optimization (compute/memory/storage-optimized)\n",
    "- Autoscaling configuration (min/max, triggers, timeouts)\n",
    "- Spot instance policies (availability zones, fallback logic)\n",
    "- Spark configuration (executor memory, parallelism, serialization)\n",
    "- JVM tuning (heap size, garbage collection)\n",
    "- Cluster policies (timeouts, permissions, init scripts)\n",
    "\n",
    "**Snowflake equivalent:** Select warehouse size (XS, S, M, L, XL, 2XL, 3XL, 4XL)\n",
    "\n",
    "| Metric | Databricks | Snowflake |\n",
    "|--------|-----------|----------|\n",
    "| Configuration decisions per cluster | 10-15+ | 1 |\n",
    "| Weekly management time (per engineer) | x+ hours | 0 hours |\n",
    "| Typical cost overrun from misconfiguration | 40-60% | ~0% |\n",
    "| Cold start time | 5-35 minutes | 1-5 seconds |\n",
    "| Expertise required | Spark/JVM expert | Basic SQL user |\n",
    "| Auto-suspend reliability | Manual tuning required | Works automatically |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã Part 1: The Configuration Decision Matrix\n",
    "\n",
    "### Every Databricks Cluster Requires These Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Driver Node Selection (Critical Decision)\n",
    "\n",
    "**Impact:** Driver undersizing causes task scheduling bottlenecks; oversizing wastes 300-400% of budget\n",
    "\n",
    "| Instance Type | vCPUs | Memory | Cost/Hour (AWS) | When to Use | Common Mistake |\n",
    "|---------------|-------|--------|-----------------|-------------|----------------|\n",
    "| **m5.xlarge** | 4 | 16 GB | $0.192 | Development, small workloads | Using in production |\n",
    "| **m5.2xlarge** | 8 | 32 GB | $0.384 | General purpose, balanced | Most common choice |\n",
    "| **m5.4xlarge** | 16 | 64 GB | $0.768 | Medium production | Underprovisioning |\n",
    "| **r5.4xlarge** | 16 | 128 GB | $1.152 | Memory-intensive aggregations | **Over-provisioning trap** |\n",
    "| **r5.8xlarge** | 32 | 256 GB | $2.304 | Very large broadcast joins | **Massive waste** |\n",
    "| **c5.4xlarge** | 16 | 32 GB | $0.680 | Compute-heavy transformations | Wrong for joins |\n",
    "\n",
    "**Real Customer Example (Unravel Data):**\n",
    "```\n",
    "‚ùå Before: r5.8xlarge driver (\"more memory = better\")\n",
    "   Monthly cost: $47,000\n",
    "   \n",
    "‚úÖ After: m5.2xlarge + optimized JVM settings  \n",
    "   Monthly cost: $18,000\n",
    "   \n",
    "Savings: $29,000/month (62% reduction)\n",
    "Root cause: Over-provisioned driver by 300-400%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Worker Node Selection\n",
    "\n",
    "**Must choose between 3 optimization categories:**\n",
    "\n",
    "| Optimization | Instance Examples | Cost/Hour | Best For | Avoid For |\n",
    "|--------------|-------------------|-----------|----------|----------|\n",
    "| **General Purpose** | m5.xlarge - m5.24xlarge | $0.19 - $4.61 | Balanced workloads | Memory-intensive |\n",
    "| **Compute Optimized** | c5.xlarge - c5.24xlarge | $0.17 - $4.08 | CPU-heavy transforms | Large joins |\n",
    "| **Memory Optimized** | r5.xlarge - r5.24xlarge | $0.25 - $6.05 | Large aggregations, joins | ETL pipelines |\n",
    "| **Storage Optimized** | i3.xlarge - i3.16xlarge | $0.31 - $4.99 | Delta cache workloads | Streaming |\n",
    "\n",
    "**Decision Framework:**\n",
    "```python\n",
    "if workload == \"BI queries with large joins\":\n",
    "    choose = \"r5 family (memory optimized)\"\n",
    "elif workload == \"ETL transformations\":\n",
    "    choose = \"m5 family (general purpose)\"\n",
    "elif workload == \"Complex calculations, ML\":\n",
    "    choose = \"c5 family (compute optimized)\"\n",
    "elif workload == \"Delta cache queries\":\n",
    "    choose = \"i3 family (storage optimized)\"\n",
    "else:\n",
    "    choose = \"m5.2xlarge (safe default, probably overpaying)\"\n",
    "```\n",
    "\n",
    "**Snowflake:** No decisions - platform auto-optimizes compute/memory/cache balance\n",
    "\n",
    "![](./dbx_decision_tree.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Autoscaling Configuration (High Complexity)\n",
    "\n",
    "**8 critical parameters requiring tuning:**\n",
    "\n",
    "| Parameter | Options | Default | Impact of Wrong Choice |\n",
    "|-----------|---------|---------|------------------------|\n",
    "| **Min Workers** | 1-256 | 2 | Too high = wasted cost, too low = slow startup |\n",
    "| **Max Workers** | 1-256 | 8 | Too low = performance bottleneck |\n",
    "| **Scale Up Trigger** | Task queue threshold | Automatic | Can't be tuned directly (black box) |\n",
    "| **Scale Up Speed** | 2-3 nodes/step | Fixed | Slow provisioning during spikes |\n",
    "| **Scale Down Delay** | 1-60 minutes | 10 min | Too aggressive = thrashing, too slow = cost waste |\n",
    "| **Scale Down Condition** | Idle threshold | \"Completely idle\" | Keeps workers longer than needed |\n",
    "| **Streaming Autoscaling** | Enabled/Disabled | Disabled | Doesn't work well for streaming |\n",
    "| **Autoscaling Mode** | Standard/Optimized | Standard | Optimized costs more DBUs |\n",
    "\n",
    "**Common Customer Complaints (Stack Overflow):**\n",
    "\n",
    "```\n",
    "‚ùì \"Why triggered when CPU usage is low?\n",
    "   Why taking 8-10 minutes to autoscale?\"\n",
    "\n",
    "üìù Answer: Autoscaling is optimized for batch jobs, not interactive queries.\n",
    "   Scale-up happens in 2 steps from min to max. Scale-down requires\n",
    "   \"completely idle for 10 minutes.\" You're paying during the 8-10 minute\n",
    "   provisioning delay.\n",
    "\n",
    "‚ùì \"Autoscaling retains executors during idle phases in streaming workloads.\"\n",
    "\n",
    "üìù Answer: Streaming autoscaling is unreliable. Recommendation: disable\n",
    "   autoscaling for streaming, use fixed cluster size.\n",
    "```\n",
    "\n",
    "**Snowflake:** Auto-suspend after X seconds of inactivity (configurable), auto-resume on query (instant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Spot Instance Configuration (Advanced)\n",
    "\n",
    "**Savings potential: 60-70% vs on-demand**  \n",
    "**Risk:** Spot interruptions cause job failures\n",
    "\n",
    "**Required Configuration Steps:**\n",
    "\n",
    "| Step | Configuration | Consequence if Wrong |\n",
    "|------|---------------|---------------------|\n",
    "| 1. Spot/On-Demand Mix | 30-70% spot recommended | 100% spot = frequent failures |\n",
    "| 2. Fallback Policy | \"Use spot with fallback to on-demand\" | No fallback = job failures |\n",
    "| 3. Availability Zones | Deploy across 3+ AZs | Single AZ = higher interruption |\n",
    "| 4. Instance Diversification | 4+ instance types | Limited types = capacity issues |\n",
    "| 5. Driver Node Policy | **ALWAYS on-demand** | Spot driver = cluster termination |\n",
    "| 6. Checkpointing Frequency | Every 15-30 minutes | No checkpoints = lost work |\n",
    "| 7. Retry Logic | Automatic restart from checkpoint | No retry = manual intervention |\n",
    "\n",
    "**Real Implementation Example (Retail Analytics Team):**\n",
    "\n",
    "```python\n",
    "# Configuration that achieved 61% cost savings:\n",
    "cluster_config = {\n",
    "    'driver_node_type': 'm5.2xlarge',  # On-demand\n",
    "    'worker_node_type': 'm5.xlarge',   # Spot + fallback\n",
    "    'spot_percentage': 70,\n",
    "    'availability_zones': ['us-east-1a', 'us-east-1b', 'us-east-1c'],\n",
    "    'instance_types': ['m5.xlarge', 'm5.2xlarge', 'm5a.xlarge', 'm5a.2xlarge'],\n",
    "    'checkpoint_interval': '20 minutes'\n",
    "}\n",
    "\n",
    "# Development time: 3 weeks\n",
    "# Required: Redesign all ETL jobs with intermediate checkpointing\n",
    "# Required: Implement automatic restart from last checkpoint\n",
    "```\n",
    "\n",
    "**Common Failure (Databricks Community):**\n",
    "```\n",
    "Error: \"Cluster was terminated during the run\"\n",
    "Cause: \"Driver node shut down by cloud provider\"\n",
    "Resolution: \"Even with 'spot with fallback to on-demand' configured,\n",
    "            if driver is spot and gets evicted, entire cluster terminates.\"\n",
    "Fix: Always use on-demand for driver nodes.\n",
    "```\n",
    "\n",
    "**Snowflake:** No spot instances, no interruptions, no configuration needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Spark Configuration Tuning (Expert Level)\n",
    "\n",
    "**Common configurations requiring manual tuning:**\n",
    "\n",
    "| Configuration | Default | Recommended | Impact if Wrong |\n",
    "|--------------|---------|-------------|----------------|\n",
    "| `spark.executor.memory` | 4g | Calculate based on worker RAM | OOM errors or underutilization |\n",
    "| `spark.executor.cores` | 4 | Match worker vCPUs | Inefficient parallelism |\n",
    "| `spark.sql.shuffle.partitions` | 200 | 2-4x total cores | Shuffle spill to disk |\n",
    "| `spark.default.parallelism` | 2x cores | 3-4x cores | Task scheduling bottleneck |\n",
    "| `spark.driver.maxResultSize` | 1g | 2-4g | \"Total size exceeds maxResultSize\" |\n",
    "| `spark.driver.memory` | 1g | 8-16g+ | Driver OOM during collect() |\n",
    "| `spark.sql.adaptive.enabled` | true | true | Poor join strategy selection |\n",
    "| `spark.sql.adaptive.coalescePartitions.enabled` | true | true | Too many output files |\n",
    "| `spark.sql.autoBroadcastJoinThreshold` | 10MB | Tune per workload | Large shuffles |\n",
    "\n",
    "**Real Failure Scenario (Stack Overflow):**\n",
    "\n",
    "```\n",
    "Problem: Query on 28K rows (x) + 23.5M rows (y) + 19 rows (z)\n",
    "         Ran in 4 minutes on old system\n",
    "         Multiple HOURS on Databricks, never completes\n",
    "         \n",
    "Root Cause: Default shuffle partitions (200) created massive overhead\n",
    "            Default broadcast threshold (10MB) caused unnecessary shuffles\n",
    "            \n",
    "Fix Required:\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"800\")  # 4x cores\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"52428800\")  # 50MB\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "**Snowflake:** Zero configuration - automatic query optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üí∞ Part 2: Cost Impact Analysis\n",
    "\n",
    "### Common Misconfiguration Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Scenario 1: Oversized Driver Node\n",
    "\n",
    "**Assumption:** 10 clusters running 12 hours/day, 22 business days/month\n",
    "\n",
    "| Configuration | Instance | $/Hour | Hours/Month | Monthly Cost | Waste |\n",
    "|---------------|----------|--------|-------------|--------------|-------|\n",
    "| ‚ùå **Oversized** | r5.8xlarge | $2.304 | 2,640 | **$6,083** | - |\n",
    "| ‚úÖ **Right-sized** | m5.2xlarge | $0.384 | 2,640 | **$1,014** | **$5,069** |\n",
    "\n",
    "**Annual waste from driver oversizing: $60,828**\n",
    "\n",
    "Add DBU markup (0.40 DBU/vCPU/hour √ó $0.55/DBU):\n",
    "- Oversized DBU cost: 32 vCPUs √ó 0.40 √ó $0.55 √ó 2,640 = $18,534\n",
    "- Right-sized DBU cost: 8 vCPUs √ó 0.40 √ó $0.55 √ó 2,640 = $4,634\n",
    "\n",
    "**Total annual waste: $75,528** (infrastructure + DBUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scenario 2: Idle Cluster Sprawl\n",
    "\n",
    "**Reality:** All-Purpose clusters \"happy to sit there and run up your bill\"\n",
    "\n",
    "**Typical organization with poor hygiene:**\n",
    "\n",
    "| Cluster Type | Count | Size | Hours Idle/Day | Daily Waste | Monthly Waste |\n",
    "|--------------|-------|------|----------------|-------------|---------------|\n",
    "| Development (forgotten) | 15 | Small | 20 | $432 | **$9,504** |\n",
    "| Testing (left running) | 8 | Medium | 16 | $614 | **$13,508** |\n",
    "| BI queries (no auto-stop) | 5 | Large | 12 | $768 | **$16,896** |\n",
    "| **Total idle waste** | | | | | **$39,908/month** |\n",
    "\n",
    "**Root cause:** \n",
    "- No automatic spending limits\n",
    "- No cost alerts\n",
    "- Auto-terminate policies not enforced\n",
    "- BI tools can't wait 1-2 minutes for startup, so endpoints run 12+ hours/day\n",
    "\n",
    "**Snowflake equivalent:** Auto-suspend after 10 minutes = $0 waste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Scenario 3: Cold Start Productivity Loss\n",
    "\n",
    "**Databricks cold start times:**\n",
    "\n",
    "| Scenario | Startup Time | Impact |\n",
    "|----------|--------------|--------|\n",
    "| Best case (pre-warmed pool) | 2-3 minutes | Acceptable for batch |\n",
    "| Normal startup | 5-6 minutes | Developer frustration |\n",
    "| Worst case (community reports) | 20-35 minutes | **\"Paying 35 minutes to run 5-minute job\"** |\n",
    "\n",
    "**Cost calculation for data science team:**\n",
    "\n",
    "```python\n",
    "team_size = 10  # data scientists\n",
    "cluster_starts_per_day = 8  # per person\n",
    "avg_startup_time = 6  # minutes\n",
    "hourly_rate = 150  # $/hour loaded cost\n",
    "\n",
    "daily_wasted_time = 10 * 8 * 6 / 60  # hours\n",
    "# = 8 hours/day wasted waiting\n",
    "\n",
    "monthly_productivity_loss = 8 * 22 * hourly_rate\n",
    "# = $26,400/month in wasted time\n",
    "\n",
    "annual_productivity_loss = monthly_productivity_loss * 12\n",
    "# = $316,800/year\n",
    "```\n",
    "\n",
    "**Snowflake:** 1-5 second resume = ~$0 productivity loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Cost Summary: Typical 50-Person Data Team\n",
    "\n",
    "| Cost Category | Databricks (Misconfigured) | Databricks (Optimized) | Snowflake | Databricks Waste |\n",
    "|---------------|---------------------------|------------------------|-----------|------------------|\n",
    "| Infrastructure (compute) | $120,000 | $75,000 | $90,000 | $30,000 |\n",
    "| Driver oversizing | $75,528 | $0 | $0 | $75,528 |\n",
    "| Idle cluster sprawl | $478,896 | $0 | $0 | $478,896 |\n",
    "| Productivity loss (cold starts) | $316,800 | $158,400 | $0 | $316,800 |\n",
    "| Optimization engineering time | $180,000 | $120,000 | $6,000 | $174,000 |\n",
    "| **Total Annual Cost** | **$1,171,224** | **$353,400** | **$96,000** | **$1,075,224** |\n",
    "\n",
    "**Key Findings:**\n",
    "- Misconfigured Databricks costs **12x more** than Snowflake\n",
    "- Even \"optimized\" Databricks costs **3.7x more** (mostly engineering time)\n",
    "- 40-60% cost overrun is conservative - reality often worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öñÔ∏è Part 3: Side-by-Side Configuration Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple BI Query Workload\n",
    "\n",
    "**Requirement:** Support 50 concurrent analysts running dashboard queries\n",
    "\n",
    "#### Databricks Configuration Required:\n",
    "\n",
    "```python\n",
    "cluster_config = {\n",
    "    # 1. Choose cluster type\n",
    "    'cluster_type': 'SQL Endpoint',  # vs All-Purpose, Jobs\n",
    "    \n",
    "    # 2. Choose warehouse size  \n",
    "    'warehouse_size': 'Large',  # 2X-Small through 4X-Large\n",
    "    \n",
    "    # 3. Configure autoscaling\n",
    "    'min_clusters': 2,  # Can't go below this\n",
    "    'max_clusters': 5,  # Hard concurrency limit\n",
    "    \n",
    "    # 4. Choose instance types (manual)\n",
    "    'driver_type': 'm5.4xlarge',  # Must research optimal\n",
    "    'worker_type': 'm5.2xlarge',  # Must research optimal\n",
    "    \n",
    "    # 5. Configure auto-stop\n",
    "    'auto_stop_mins': 120,  # Can't be aggressive (cold start cost)\n",
    "    \n",
    "    # 6. Channel configuration\n",
    "    'channel': 'CHANNEL_NAME_CURRENT',  # vs PREVIEW (breaks things)\n",
    "    \n",
    "    # 7. Photon acceleration\n",
    "    'enable_photon': True,  # Extra DBU cost\n",
    "    \n",
    "    # 8. Serverless vs Classic\n",
    "    'serverless': True,  # 2x faster startup but higher DBU cost\n",
    "    \n",
    "    # 9. Spot instances (workers only)\n",
    "    'spot_instance_policy': 'COST_OPTIMIZED',  # Risk interruptions\n",
    "    \n",
    "    # 10. Tags for cost allocation\n",
    "    'tags': {\n",
    "        'team': 'analytics',\n",
    "        'cost_center': '12345',\n",
    "        'environment': 'production'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Result: ~10 queries per cluster * 5 clusters = 50 concurrent queries\n",
    "# Problem: Hard limit at 50, no overflow handling\n",
    "# Problem: Cold start = 2-6 seconds (serverless) or 5+ minutes (classic)\n",
    "# Problem: Must monitor and adjust manually\n",
    "```\n",
    "\n",
    "**Monthly cost:** \n",
    "- Infrastructure: ~$8,000 (running 12 hrs/day)\n",
    "- DBUs (Serverless): ~$12,000\n",
    "- **Total: $20,000/month**\n",
    "\n",
    "#### Snowflake Configuration Required:\n",
    "\n",
    "```sql\n",
    "-- 1. Create warehouse\n",
    "CREATE WAREHOUSE analytics_wh\n",
    "  WITH WAREHOUSE_SIZE = 'LARGE'\n",
    "  AUTO_SUSPEND = 600  -- 10 minutes\n",
    "  AUTO_RESUME = TRUE\n",
    "  INITIALLY_SUSPENDED = TRUE;\n",
    "\n",
    "-- 2. Optional: Enable multi-cluster for concurrency\n",
    "ALTER WAREHOUSE analytics_wh SET\n",
    "  MIN_CLUSTER_COUNT = 1\n",
    "  MAX_CLUSTER_COUNT = 5\n",
    "  SCALING_POLICY = 'STANDARD';\n",
    "\n",
    "-- That's it. 2 SQL statements.\n",
    "```\n",
    "\n",
    "**Monthly cost:**\n",
    "- Large warehouse: $4/credit/hour\n",
    "- Actual runtime: ~8 hours/day (auto-suspend aggressive)\n",
    "- **Total: ~$7,000/month**\n",
    "\n",
    "**Savings: $13,000/month (65% cheaper)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ETL Pipeline Workload\n",
    "\n",
    "**Requirement:** Nightly batch processing, 500GB data, 2-hour window\n",
    "\n",
    "#### Databricks Configuration:\n",
    "\n",
    "```python\n",
    "job_cluster_config = {\n",
    "    # Must choose job cluster (not all-purpose, 2x cost difference)\n",
    "    'cluster_type': 'Job Cluster',  # $0.277/DBU vs $0.508/DBU\n",
    "    \n",
    "    # Worker configuration\n",
    "    'num_workers': 16,  # Or use autoscaling\n",
    "    'worker_type': 'm5.2xlarge',  # 8 vCPU, 32 GB\n",
    "    \n",
    "    # Driver configuration  \n",
    "    'driver_type': 'm5.4xlarge',  # Need larger for shuffle\n",
    "    \n",
    "    # Spark tuning for this workload\n",
    "    'spark_conf': {\n",
    "        'spark.sql.shuffle.partitions': '512',  # 4x cores\n",
    "        'spark.executor.memory': '24g',  # Leave 8GB for OS\n",
    "        'spark.driver.memory': '48g',\n",
    "        'spark.driver.maxResultSize': '8g',\n",
    "        'spark.sql.adaptive.enabled': 'true',\n",
    "        'spark.sql.adaptive.coalescePartitions.enabled': 'true'\n",
    "    },\n",
    "    \n",
    "    # Cost optimization\n",
    "    'spot_instances': {\n",
    "        'worker_spot_bid_price_percent': 100,\n",
    "        'enable_spot_fallback': True,\n",
    "        'availability_zones': ['us-east-1a', 'us-east-1b', 'us-east-1c']\n",
    "    },\n",
    "    \n",
    "    # Reliability\n",
    "    'max_retries': 3,\n",
    "    'timeout_seconds': 14400  # 4 hours\n",
    "}\n",
    "\n",
    "# Startup overhead: 5-6 minutes\n",
    "# Actual processing: 2 hours\n",
    "# Total billable: 2 hours 6 minutes\n",
    "```\n",
    "\n",
    "**Nightly cost:**\n",
    "- Infrastructure: 16 √ó $0.384 √ó 2.1 = $12.90\n",
    "- Driver: $0.768 √ó 2.1 = $1.61  \n",
    "- DBUs: (16√ó8 + 16) √ó 0.277 √ó $0.55 √ó 2.1 = $78.12\n",
    "- **Total per run: $92.63**\n",
    "- **Monthly (22 runs): $2,038**\n",
    "\n",
    "#### Snowflake Configuration:\n",
    "\n",
    "```sql\n",
    "-- Create warehouse for ETL\n",
    "CREATE WAREHOUSE etl_wh\n",
    "  WITH WAREHOUSE_SIZE = 'XLARGE'\n",
    "  AUTO_SUSPEND = 60\n",
    "  AUTO_RESUME = TRUE;\n",
    "\n",
    "-- Run ETL in scheduled task\n",
    "CREATE TASK nightly_etl\n",
    "  WAREHOUSE = etl_wh\n",
    "  SCHEDULE = 'USING CRON 0 2 * * * America/New_York'\n",
    "AS\n",
    "  CALL etl_procedure();\n",
    "\n",
    "-- Startup: Instant\n",
    "-- Processing: 1.5 hours (more efficient engine)\n",
    "```\n",
    "\n",
    "**Nightly cost:**\n",
    "- XLarge: 16 credits/hour\n",
    "- Runtime: 1.5 hours\n",
    "- Cost: 16 √ó 1.5 √ó $4 = $96\n",
    "- **Monthly (22 runs): $2,112**\n",
    "\n",
    "**Comparison:** Nearly identical cost, but:\n",
    "- Databricks: 10-15 config decisions, manual tuning required\n",
    "- Snowflake: 2 SQL statements, works automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Configuration Complexity Score\n",
    "\n",
    "| Workload Type | Databricks Decisions | Databricks Expertise | Snowflake Decisions | Snowflake Expertise |\n",
    "|---------------|---------------------|---------------------|--------------------|-----------------|\n",
    "| **BI Dashboard** | 10+ | Spark expert | 1 | SQL user |\n",
    "| **Ad-hoc Analytics** | 8+ | Spark expert | 1 | SQL user |\n",
    "| **ETL Pipeline** | 12+ | Spark + JVM expert | 1 | SQL user |\n",
    "| **Streaming** | 15+ | Spark + Streaming expert | 2 | SQL user |\n",
    "| **ML Training** | 18+ | Spark + ML + GPU expert | 2-3 | SQL + Python |\n",
    "\n",
    "**Time investment per cluster configuration:**\n",
    "- Databricks (first time): 4-8 hours research + testing\n",
    "- Databricks (ongoing tuning): 2-3 hours/week\n",
    "- Snowflake (first time): 5-10 minutes\n",
    "- Snowflake (ongoing tuning): 0 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Part 4: Competitive Positioning Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Discovery Questions\n",
    "\n",
    "**Ask these to uncover Databricks compute pain:**\n",
    "\n",
    "1. **\"How do you currently size your Databricks clusters?\"**\n",
    "   - Listen for: Trial and error, over-provisioning \"to be safe\", past performance issues\n",
    "   \n",
    "2. **\"How long does it take for your team to start running queries in the morning?\"**\n",
    "   - Listen for: Cold start delays, complaints about waiting\n",
    "   \n",
    "3. **\"Do you know what your idle cluster costs are?\"**\n",
    "   - Listen for: \"Not really\", \"probably significant\", \"we have policies but...\"\n",
    "   \n",
    "4. **\"How much time does your team spend on cluster optimization?\"**\n",
    "   - Listen for: Weekly rituals, dedicated \"performance engineering\" roles\n",
    "   \n",
    "5. **\"Have you had any surprises in your Databricks bills?\"**\n",
    "   - Listen for: \"Every month\", specific spike stories, forensics to understand costs\n",
    "   \n",
    "6. **\"What happens when more users than expected hit your system?\"**\n",
    "   - Listen for: Autoscaling delays, hard limits, user complaints\n",
    "   \n",
    "7. **\"Who on your team understands how to tune Spark configurations?\"**\n",
    "   - Listen for: Single point of failure, \"our Spark expert\", hiring struggles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Customer Pain Point Mapping\n",
    "\n",
    "| Customer Says... | Root Cause | Snowflake Solution |\n",
    "|------------------|------------|--------------------|\n",
    "| \"Our Databricks costs keep going up\" | Idle clusters, oversized drivers, inefficient autoscaling | Auto-suspend + right-sized warehouses |\n",
    "| \"BI dashboards are slow to load\" | Cold start times (5+ minutes) | 1-5 second resume |\n",
    "| \"We need a Spark expert on every team\" | Configuration complexity | SQL users can manage warehouses |\n",
    "| \"Autoscaling doesn't work as expected\" | Black-box scaling logic, slow provisioning | Transparent multi-cluster with predictable behavior |\n",
    "| \"We spend hours tuning clusters weekly\" | No automatic optimization | Zero tuning required |\n",
    "| \"Queries fail when we hit concurrency limits\" | Hard 10 queries/cluster limit | Elastic scaling to 100s of queries |\n",
    "| \"Cost attribution is impossible\" | Cluster tags don't propagate, job names change | Query-level cost visibility built-in |\n",
    "| \"Spot instance interruptions break pipelines\" | Complex spot configuration required | No spot instances, no interruptions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Objection Handling\n",
    "\n",
    "**Objection:** *\"Databricks is more flexible - we can tune everything\"*\n",
    "\n",
    "**Response:** \"That's true, but flexibility comes with a cost. Your team spends X+ hours per week on cluster optimization. That's $180,000/year in engineering time for a 10-person team. Snowflake's 'opinionated' approach eliminates that overhead while delivering better performance for SQL workloads. When do you actually need that flexibility versus when is it just added complexity?\"\n",
    "\n",
    "---\n",
    "\n",
    "**Objection:** *\"We're already optimized, only using Job clusters and spot instances\"*\n",
    "\n",
    "**Response:** \"That's great that you've invested in optimization. But even optimized Databricks requires:\n",
    "- Ongoing cluster tuning (2-3 hours/week per engineer)\n",
    "- Cold start delays (5-6 minutes vs instant for Snowflake)\n",
    "- Spot interruption handling (checkpointing, retry logic)\n",
    "- Expert knowledge for each workload type\n",
    "\n",
    "Snowflake delivers similar or better performance with zero ongoing maintenance. What could your team build with those recovered hours?\"\n",
    "\n",
    "---\n",
    "\n",
    "**Objection:** *\"Serverless SQL fixes the cluster management issues\"*\n",
    "\n",
    "**Response:** \"Serverless SQL is definitely an improvement, but it:\n",
    "- Still has the 10 queries/cluster hard limit\n",
    "- Costs 2x more in DBUs than classic\n",
    "- Has 2-6 second cold start (vs <1 second for Snowflake)\n",
    "- Requires warehouse sizing decisions\n",
    "- Doesn't help with ETL/ML workloads (not supported)\n",
    "\n",
    "It's a band-aid on the architectural complexity, not a solution.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Objection:** *\"We need unified analytics - ML and SQL together\"*\n",
    "\n",
    "**Response:** \"Absolutely, and if you need both, Databricks makes sense. But let's be honest about the SQL side:\n",
    "- What % of your users actually use notebooks vs SQL?\n",
    "- How much of your compute budget goes to SQL workloads?\n",
    "- What's the cost of this unified platform for your SQL users?\n",
    "\n",
    "Many customers find that using Snowflake for SQL + Databricks for ML is actually cheaper and simpler than trying to do everything in Databricks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Part 5: Quick Reference Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Cluster Type Decision Matrix\n",
    "\n",
    "| Cluster Type | Cost (DBU) | Startup Time | Best For | Avoid For |\n",
    "|--------------|-----------|--------------|----------|----------|\n",
    "| **All-Purpose** | $0.508/DBU | 5-6 min | Interactive dev/testing | Production (2x cost) |\n",
    "| **Job Cluster** | $0.277/DBU | 5-6 min | Scheduled ETL | Interactive queries |\n",
    "| **SQL Endpoint (Classic)** | $0.40/DBU | 5+ min | BI dashboards | Development |\n",
    "| **SQL Endpoint (Serverless)** | $0.80/DBU | 2-6 sec | High-concurrency BI | Cost-sensitive workloads |\n",
    "| **Instance Pool** | Infrastructure only | 2-3 min | Frequent starts | Long-running jobs |\n",
    "\n",
    "**Snowflake:** Single warehouse type, scales automatically for all workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 AWS Instance Quick Reference\n",
    "\n",
    "| Family | Example | vCPU | RAM | $/Hour | Use Case |\n",
    "|--------|---------|------|-----|--------|----------|\n",
    "| **m5 (General)** | m5.2xlarge | 8 | 32 GB | $0.384 | Default choice, balanced |\n",
    "| **m5 (General)** | m5.4xlarge | 16 | 64 GB | $0.768 | Medium workloads |\n",
    "| **c5 (Compute)** | c5.4xlarge | 16 | 32 GB | $0.680 | CPU-intensive transforms |\n",
    "| **r5 (Memory)** | r5.4xlarge | 16 | 128 GB | $1.152 | Large joins/aggregations |\n",
    "| **r5 (Memory)** | r5.8xlarge | 32 | 256 GB | $2.304 | **Over-provisioning trap** |\n",
    "| **i3 (Storage)** | i3.2xlarge | 8 | 61 GB | $0.624 | Delta cache queries |\n",
    "\n",
    "**Snowflake:** No instance selection needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Troubleshooting Common Issues\n",
    "\n",
    "| Symptom | Likely Cause | Databricks Fix | Snowflake Experience |\n",
    "|---------|--------------|----------------|----------------------|\n",
    "| Queries taking 5+ minutes to start | Cold cluster startup | Use instance pools or serverless | <5 second resume |\n",
    "| \"Total size exceeds maxResultSize\" | Driver memory too small | Increase `spark.driver.maxResultSize` | Doesn't happen |\n",
    "| OOM errors during aggregation | Driver undersized | Upgrade to r5.4xlarge+ | Automatic memory management |\n",
    "| Autoscaling too slow | Provisioning delay | Pre-warm instance pools | Multi-cluster instant |\n",
    "| Query waiting in queue | Hit 10 query/cluster limit | Add more clusters | Elastic scaling |\n",
    "| Spot instance failures | Worker interruption | Switch to on-demand or add fallback | No spot instances |\n",
    "| Slow shuffle operations | Wrong partition count | Tune `spark.sql.shuffle.partitions` | Automatic partitioning |\n",
    "| High costs from idle clusters | No auto-terminate | Enforce cluster policies | Auto-suspend works reliably |\n",
    "| BI dashboard delays | Cold start every query | Keep endpoint running ($$) | Instant resume ($) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Conclusion: The Operational Burden\n",
    "\n",
    "### Summary of Hidden Costs\n",
    "\n",
    "**For a typical 50-person data team:**\n",
    "\n",
    "| Cost Category | Annual Amount | What It Represents |\n",
    "|---------------|---------------|--------------------|\n",
    "| Optimization engineering time | $375,000 | 10 engineers √ó 10 hrs/week √ó $150/hr |\n",
    "| Productivity loss (cold starts) | $288,000 | 50 analysts √ó 8 starts/day √ó 6 min wait |\n",
    "| Misconfiguration waste | $500,000+ | Driver oversizing, idle clusters, wrong instance types |\n",
    "| **Total Hidden Overhead** | **$1,163,000** | **Cost beyond listed DBU pricing** |\n",
    "\n",
    "**Snowflake equivalent overhead:** ~$6,000 (minimal warehouse management)\n",
    "\n",
    "**Annual savings:** ~$1,157,000\n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "Databricks compute management requires:\n",
    "- **Expert knowledge** across Spark, JVM, cloud infrastructure\n",
    "- **Continuous optimization** consuming 10+ engineering hours/week\n",
    "- **Complex decision-making** for every cluster (10-15 parameters)\n",
    "- **Ongoing monitoring** to prevent cost overruns\n",
    "- **Trial-and-error tuning** for each workload type\n",
    "\n",
    "Snowflake warehouse management requires:\n",
    "- **One decision:** What size? (XS through 4XL)\n",
    "- **Zero ongoing optimization** - automatic tuning\n",
    "- **Instant scaling** with no cold start delays\n",
    "- **Built-in cost controls** with resource monitors\n",
    "- **Predictable costs** with per-second billing\n",
    "\n",
    "### When to Use This Information\n",
    "\n",
    "**This comparison is most powerful when:**\n",
    "1. Customer mentions Databricks complexity or tuning challenges\n",
    "2. Engineering team size is small relative to data team needs\n",
    "3. BI/analytics is primary use case (not ML/streaming)\n",
    "4. Cost predictability is a concern\n",
    "5. Customer has experienced \"sticker shock\" on Databricks bills\n",
    "\n",
    "**Remember:** Databricks excels at ML, streaming, and complex data engineering. This notebook focuses specifically on the SQL analytics / BI query workload where Snowflake's architecture provides clear advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Additional Resources\n",
    "\n",
    "**For deeper dives:**\n",
    "- Unravel Data: \"Which Instance Types And Cluster Settings Most Affect Databricks Costs?\"\n",
    "- Sync Computing: \"Databricks Compute Comparison: Classic Jobs vs Serverless Jobs vs SQL Warehouses\"\n",
    "- Stack Overflow: Search \"databricks cluster optimization\" for real customer pain\n",
    "- Databricks Community: Filter by \"cluster configuration\" tag\n",
    "\n",
    "**Internal Snowflake resources:**\n",
    "- TCO calculator with compute complexity modeling\n",
    "- Customer migration case studies (Databricks ‚Üí Snowflake)\n",
    "- Battle cards: \"Databricks Compute vs Snowflake Warehouses\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
