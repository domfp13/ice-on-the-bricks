{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58540a4d-4920-442f-aa82-3bf6259843b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Delta Lake Protocol Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c4fdec3-17ac-45e6-929c-7cac206d43c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###How Delta Lake Protocol Versioning Works?\n",
    "Every Delta table stores its protocol version in the **_delta_log/** transaction log. This version determines which features the table supports and which Databricks Runtime/Spark versions can read/write it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd84bfb5-61b1-415f-a20d-37670de89561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Compatibility Rules\n",
    "Delta Lake is backwards compatible - newer versions can always read/write older tables. However, enabling some features breaks forward compatibility with older versions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad5f742-5e87-4947-8d00-d37c776d16eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SETUP:\n",
    "# 1. Create TWO clusters with different Databricks Runtime versions:\n",
    "#    - Cluster A: Runtime 11.3 LTS (older)\n",
    "#    - Cluster B: Runtime 13.3 LTS or higher (newer)\n",
    "# 2. Run steps sequentially as indicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba9e4d6-16de-47a7-ae98-49e364fd6c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# STEP 1: Create table on OLD cluster (Runtime 11.3)\n",
    "# Run this on Cluster A (Runtime 11.3 LTS)\n",
    "# ========================================\n",
    "\n",
    "# Create a simple test table\n",
    "TABLE_NAME = \"hive_metastore.default.protocol_test_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79037cf6-f271-4cd9-8b5a-4a7b1fd1b7d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nPROTOCOL BREAKING CHANGE TEST\n============================================================\n\n[STEP 1] Creating table on OLD Runtime cluster...\nTable: hive_metastore.default.protocol_test_table\n✅ Table created successfully\n\n[PROTOCOL CHECK] Initial protocol version:\n  minReaderVersion: 1\n  minWriterVersion: 2\n\n[TEST] Can read from table:\n+---+-------+------+\n| id|   name|amount|\n+---+-------+------+\n|  3|Charlie|   300|\n|  1|  Alice|   100|\n|  2|    Bob|   200|\n+---+-------+------+\n\n\n[TEST] Can write to table:\n✅ Write successful on Runtime 11.3\n\n============================================================\nSTEP 1 COMPLETE - Now switch to Cluster B (Runtime 13.3+)\n============================================================\n"
     ]
    }
   ],
   "source": [
    "# Delta Lake Protocol Breaking Change Demonstration\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PROTOCOL BREAKING CHANGE TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(f\"\\n[STEP 1] Creating table on OLD Runtime cluster...\")\n",
    "print(f\"Table: {TABLE_NAME}\")\n",
    "\n",
    "# Create initial data\n",
    "data = [\n",
    "    (1, \"Alice\", 100),\n",
    "    (2, \"Bob\", 200),\n",
    "    (3, \"Charlie\", 300)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Write as Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLE_NAME)\n",
    "\n",
    "print(\"✅ Table created successfully\")\n",
    "\n",
    "# Check the protocol version\n",
    "print(\"\\n[PROTOCOL CHECK] Initial protocol version:\")\n",
    "protocol_info = spark.sql(f\"DESCRIBE DETAIL {TABLE_NAME}\").select(\"minReaderVersion\", \"minWriterVersion\").collect()[0]\n",
    "print(f\"  minReaderVersion: {protocol_info['minReaderVersion']}\")\n",
    "print(f\"  minWriterVersion: {protocol_info['minWriterVersion']}\")\n",
    "\n",
    "# Verify read/write works\n",
    "print(\"\\n[TEST] Can read from table:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}\").show()\n",
    "\n",
    "print(\"\\n[TEST] Can write to table:\")\n",
    "spark.sql(f\"INSERT INTO {TABLE_NAME} VALUES (4, 'David', 400)\")\n",
    "print(\"✅ Write successful on Runtime 11.3\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 1 COMPLETE - Now switch to Cluster B (Runtime 13.3+)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a63f6ffd-0c55-49da-b334-1fa7dd1e8419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# STEP 2: Enable feature (enableDeletionVectors) that breaks compatibility\n",
    "# Run this on Cluster B (Runtime 13.3 LTS or higher)\n",
    "# ========================================\n",
    "\n",
    "# Create a simple test table\n",
    "TABLE_NAME = \"hive_metastore.default.protocol_test_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f35eaf-9e46-4179-a856-84ed076e8cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[STEP 2] Running on NEWER Runtime cluster...\n\n[TEST] Verify table access before upgrade:\n  Current row count: 4\n\n[PROTOCOL CHECK] Before enabling deletion vectors:\n  minReaderVersion: 1\n  minWriterVersion: 2\n\n[BREAKING CHANGE] Enabling deletion vectors...\n✅ Deletion vectors enabled\n\n[PROTOCOL CHECK] After enabling deletion vectors:\n  minReaderVersion: 3\n  minWriterVersion: 7\n\n\uD83D\uDEA8 PROTOCOL UPGRADED FROM (1,2) TO (3,7)\n\n[TEST] Performing DELETE operation to create deletion vectors:\n✅ DELETE successful on Runtime 13.3+\n\n[TEST] Table contents after delete:\n+---+-------+------+\n| id|   name|amount|\n+---+-------+------+\n|  3|Charlie|   300|\n|  4|  David|   400|\n|  2|    Bob|   200|\n+---+-------+------+\n\n\n============================================================\nSTEP 2 COMPLETE - Now switch BACK to Cluster A (Runtime 11.3)\n============================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n[STEP 2] Running on NEWER Runtime cluster...\")\n",
    "\n",
    "# First, verify we can still access the table\n",
    "print(\"\\n[TEST] Verify table access before upgrade:\")\n",
    "current_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {TABLE_NAME}\").collect()[0]['cnt']\n",
    "print(f\"  Current row count: {current_count}\")\n",
    "\n",
    "# Check current protocol\n",
    "print(\"\\n[PROTOCOL CHECK] Before enabling deletion vectors:\")\n",
    "protocol_before = spark.sql(f\"DESCRIBE DETAIL {TABLE_NAME}\").select(\"minReaderVersion\", \"minWriterVersion\").collect()[0]\n",
    "print(f\"  minReaderVersion: {protocol_before['minReaderVersion']}\")\n",
    "print(f\"  minWriterVersion: {protocol_before['minWriterVersion']}\")\n",
    "\n",
    "# NOW BREAK COMPATIBILITY by enabling deletion vectors\n",
    "print(\"\\n[BREAKING CHANGE] Enabling deletion vectors...\")\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {TABLE_NAME} \n",
    "SET TBLPROPERTIES ('delta.enableDeletionVectors' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Deletion vectors enabled\")\n",
    "\n",
    "# Check protocol after upgrade\n",
    "print(\"\\n[PROTOCOL CHECK] After enabling deletion vectors:\")\n",
    "protocol_after = spark.sql(f\"DESCRIBE DETAIL {TABLE_NAME}\").select(\"minReaderVersion\", \"minWriterVersion\").collect()[0]\n",
    "print(f\"  minReaderVersion: {protocol_after['minReaderVersion']}\")\n",
    "print(f\"  minWriterVersion: {protocol_after['minWriterVersion']}\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDEA8 PROTOCOL UPGRADED FROM ({protocol_before['minReaderVersion']},{protocol_before['minWriterVersion']}) TO ({protocol_after['minReaderVersion']},{protocol_after['minWriterVersion']})\")\n",
    "\n",
    "# Perform a DELETE operation (creates deletion vectors)\n",
    "print(\"\\n[TEST] Performing DELETE operation to create deletion vectors:\")\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE id = 1\")\n",
    "print(\"✅ DELETE successful on Runtime 13.3+\")\n",
    "\n",
    "# Show table contents\n",
    "print(\"\\n[TEST] Table contents after delete:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2 COMPLETE - Now switch BACK to Cluster A (Runtime 11.3)\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac468ecb-bd0a-4ac2-879c-09976c6bbda8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# STEP 3: Run this on Cluster A (Runtime 11.3 LTS)\n",
    "# ========================================\n",
    "\n",
    "# Create a simple test table\n",
    "TABLE_NAME = \"hive_metastore.default.protocol_test_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7a3b047-b940-43b9-9ad1-ba50d5690482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[STEP 3] Attempting to access table from OLD Runtime cluster...\n\n[TEST] Trying to READ from table:\n\n\uD83D\uDEA8 READ FAILED (Expected):\n   Error: An error occurred while calling o368.sql.\n: com.databricks.sql.transaction.tahoe.DeltaTableFeatureException: Unable to read this table because it requires reader table feature(s) that are unsupported ...\n   ✅ Confirmed: Protocol version incompatibility!\n\n[TEST] Trying to WRITE to table:\n\n\uD83D\uDEA8 WRITE FAILED (Expected):\n   Error: An error occurred while calling o368.sql.\n: com.databricks.sql.transaction.tahoe.DeltaTableFeatureException: Unable to read this table because it requires reader table feature(s) that are unsupported ...\n   ✅ Confirmed: Protocol version incompatibility!\n\n============================================================\nTEST COMPLETE\n============================================================\n\nSUMMARY:\n1. Created table on Runtime 11.3 with protocol (1,2)\n2. Upgraded protocol to (3,7) by enabling deletion vectors on Runtime 13.3+\n3. Runtime 11.3 can NO LONGER access the table\n\nThis demonstrates the IRREVERSIBLE nature of protocol upgrades and \nthe compatibility issues between different Runtime versions.\n\nSNOWFLAKE COMPARISON:\n- No version management needed\n- All features available to all users immediately\n- No risk of breaking existing queries/jobs\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n[STEP 3] Attempting to access table from OLD Runtime cluster...\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n[TEST] Trying to READ from table:\")\n",
    "    result = spark.sql(f\"SELECT * FROM {TABLE_NAME}\")\n",
    "    result.show()\n",
    "    print(\"❌ UNEXPECTED: Read should have failed but didn't\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"\\n\uD83D\uDEA8 READ FAILED (Expected):\")\n",
    "    print(f\"   Error: {error_msg[:200]}...\")\n",
    "    if \"protocol\" in error_msg.lower() or \"version\" in error_msg.lower():\n",
    "        print(\"   ✅ Confirmed: Protocol version incompatibility!\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n[TEST] Trying to WRITE to table:\")\n",
    "    spark.sql(f\"INSERT INTO {TABLE_NAME} VALUES (5, 'Eve', 500)\")\n",
    "    print(\"❌ UNEXPECTED: Write should have failed but didn't\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"\\n\uD83D\uDEA8 WRITE FAILED (Expected):\")\n",
    "    print(f\"   Error: {error_msg[:200]}...\")\n",
    "    if \"protocol\" in error_msg.lower() or \"version\" in error_msg.lower():\n",
    "        print(\"   ✅ Confirmed: Protocol version incompatibility!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "SUMMARY:\n",
    "1. Created table on Runtime 11.3 with protocol (1,2)\n",
    "2. Upgraded protocol to (3,7) by enabling deletion vectors on Runtime 13.3+\n",
    "3. Runtime 11.3 can NO LONGER access the table\n",
    "\n",
    "This demonstrates the IRREVERSIBLE nature of protocol upgrades and \n",
    "the compatibility issues between different Runtime versions.\n",
    "\n",
    "SNOWFLAKE COMPARISON:\n",
    "- No version management needed\n",
    "- All features available to all users immediately\n",
    "- No risk of breaking existing queries/jobs\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b00a49-355b-4823-840a-3bfd73013cfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nBONUS: Inspect Delta Transaction Log\n============================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "\u001B[0;32m<command-7522936539118669>\u001B[0m in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[0;31m# Get table location\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 11\u001B[0;31m \u001B[0mtable_location\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"DESCRIBE DETAIL {TABLE_NAME}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"location\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"\\nTable location: {table_location}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Transaction log: {table_location}/_delta_log/\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36msql\u001B[0;34m(self, sqlQuery, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1117\u001B[0m             \u001B[0msqlQuery\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformatter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1118\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1119\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   1120\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1121\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n",
       "\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o368.sql.\n",
       ": com.databricks.sql.transaction.tahoe.DeltaTableFeatureException: Unable to read this table because it requires reader table feature(s) that are unsupported by this version of Databricks: deletionVectors. Please refer to https://docs.microsoft.com/azure/databricks/delta/feature-compatibility for more information on Delta Lake feature compatibility.\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.unsupportedReaderTableFeaturesInTableException(DeltaErrors.scala:2107)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.unsupportedReaderTableFeaturesInTableException$(DeltaErrors.scala:2101)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.unsupportedReaderTableFeaturesInTableException(DeltaErrors.scala:2534)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.$anonfun$protocolCheck$4(DeltaLog.scala:424)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.protocolCheck(DeltaLog.scala:442)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.protocolRead(DeltaLog.scala:508)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotEdge.init(SnapshotEdge.scala:197)\n",
       "\tat com.databricks.sql.transaction.tahoe.Snapshot.<init>(Snapshot.scala:528)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotEdge.<init>(SnapshotEdge.scala:82)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.$anonfun$createSnapshot$5(SnapshotManagementEdge.scala:207)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:481)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:469)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.$anonfun$createSnapshot$1(SnapshotManagementEdge.scala:193)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.createSnapshot(SnapshotManagementEdge.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.createSnapshot$(SnapshotManagementEdge.scala:184)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.createSnapshot(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$installLogSegmentInternal$1(SnapshotManagement.scala:672)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$installLogSegmentInternal$1$adapted(SnapshotManagement.scala:662)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.installLogSegmentInternal(SnapshotManagement.scala:662)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.installLogSegmentInternal$(SnapshotManagement.scala:657)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.installLogSegmentInternal(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$updateInternal$1(SnapshotManagement.scala:653)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:236)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:223)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.withOperationTypeTag(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:161)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:305)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:303)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:160)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:547)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:404)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:402)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:399)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:447)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:432)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:638)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:556)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:547)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:517)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.recordOperation(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:159)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:149)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:136)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.recordDeltaOperation(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.updateInternal(SnapshotManagement.scala:648)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.updateInternal$(SnapshotManagement.scala:647)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.com$databricks$sql$transaction$tahoe$SnapshotManagementEdge$$super$updateInternal(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.updateInternal(SnapshotManagementEdge.scala:163)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.updateInternal$(SnapshotManagementEdge.scala:160)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.updateInternal(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$update$7(SnapshotManagement.scala:603)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.lockInterruptibly(DeltaLog.scala:204)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$update$6(SnapshotManagement.scala:602)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:305)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:303)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.update(SnapshotManagement.scala:602)\n",
       "\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.update$(SnapshotManagement.scala:543)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.update(DeltaLog.scala:81)\n",
       "\tat com.databricks.sql.DatabricksSessionCatalog.$anonfun$getDeltaMetadata$3(DatabricksSessionCatalog.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:759)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1698)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:760)\n",
       "\tat com.databricks.sql.DatabricksSessionCatalog.getDeltaMetadata(DatabricksSessionCatalog.scala:166)\n",
       "\tat com.databricks.sql.DatabricksSessionCatalog.getTableRawMetadata(DatabricksSessionCatalog.scala:87)\n",
       "\tat com.databricks.sql.DatabricksSessionCatalog.getTableMetadata(DatabricksSessionCatalog.scala:93)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.isDeltaTable(DeltaTable.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableIdentifier$.apply(DeltaTableIdentifier.scala:108)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaAnalysisEdge$$anonfun$apply0$1.applyOrElse(DeltaAnalysisEdge.scala:269)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaAnalysisEdge$$anonfun$apply0$1.applyOrElse(DeltaAnalysisEdge.scala:102)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:354)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:31)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaAnalysisEdge.apply0(DeltaAnalysisEdge.scala:102)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaAnalysisEdge.apply(DeltaAnalysisEdge.scala:70)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaAnalysisEdge.apply(DeltaAnalysisEdge.scala:57)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:342)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:315)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:314)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:175)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:344)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:373)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:814)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:367)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:364)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:995)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:364)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:169)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:169)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:161)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:107)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:995)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:105)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:830)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:995)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:825)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-7522936539118669>\u001B[0m in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m# Get table location\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mtable_location\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"DESCRIBE DETAIL {TABLE_NAME}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"location\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"\\nTable location: {table_location}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Transaction log: {table_location}/_delta_log/\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36msql\u001B[0;34m(self, sqlQuery, **kwargs)\u001B[0m\n\u001B[1;32m   1117\u001B[0m             \u001B[0msqlQuery\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformatter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1118\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1119\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1120\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1121\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o368.sql.\n: com.databricks.sql.transaction.tahoe.DeltaTableFeatureException: Unable to read this table because it requires reader table feature(s) that are unsupported by this version of Databricks: deletionVectors. Please refer to https://docs.microsoft.com/azure/databricks/delta/feature-compatibility for more information on Delta Lake feature compatibility.\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.unsupportedReaderTableFeaturesInTableException(DeltaErrors.scala:2107)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.unsupportedReaderTableFeaturesInTableException$(DeltaErrors.scala:2101)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.unsupportedReaderTableFeaturesInTableException(DeltaErrors.scala:2534)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.$anonfun$protocolCheck$4(DeltaLog.scala:424)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.protocolCheck(DeltaLog.scala:442)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.protocolRead(DeltaLog.scala:508)\n\tat com.databricks.sql.transaction.tahoe.SnapshotEdge.init(SnapshotEdge.scala:197)\n\tat com.databricks.sql.transaction.tahoe.Snapshot.<init>(Snapshot.scala:528)\n\tat com.databricks.sql.transaction.tahoe.SnapshotEdge.<init>(SnapshotEdge.scala:82)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.$anonfun$createSnapshot$5(SnapshotManagementEdge.scala:207)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:481)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:469)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:81)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.$anonfun$createSnapshot$1(SnapshotManagementEdge.scala:193)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.createSnapshot(SnapshotManagementEdge.scala:189)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.createSnapshot$(SnapshotManagementEdge.scala:184)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.createSnapshot(DeltaLog.scala:81)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$installLogSegmentInternal$1(SnapshotManagement.scala:672)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$installLogSegmentInternal$1$adapted(SnapshotManagement.scala:662)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.installLogSegmentInternal(SnapshotManagement.scala:662)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.installLogSegmentInternal$(SnapshotManagement.scala:657)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.installLogSegmentInternal(DeltaLog.scala:81)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$updateInternal$1(SnapshotManagement.scala:653)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:236)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:223)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withOperationTypeTag(DeltaLog.scala:81)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:161)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:305)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:303)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:81)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:160)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:547)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:643)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:664)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:404)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:402)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:399)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:447)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:432)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:638)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:556)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:547)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:517)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.recordOperation(DeltaLog.scala:81)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:159)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:149)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:136)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.recordDeltaOperation(DeltaLog.scala:81)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.updateInternal(SnapshotManagement.scala:648)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.updateInternal$(SnapshotManagement.scala:647)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.com$databricks$sql$transaction$tahoe$SnapshotManagementEdge$$super$updateInternal(DeltaLog.scala:81)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.updateInternal(SnapshotManagementEdge.scala:163)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.updateInternal$(SnapshotManagementEdge.scala:160)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.updateInternal(DeltaLog.scala:81)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$update$7(SnapshotManagement.scala:603)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.lockInterruptibly(DeltaLog.scala:204)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$update$6(SnapshotManagement.scala:602)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:305)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:303)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:81)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.update(SnapshotManagement.scala:602)\n\tat com.databricks.sql.transaction.tahoe.SnapshotManagement.update$(SnapshotManagement.scala:543)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.update(DeltaLog.scala:81)\n\tat com.databricks.sql.DatabricksSessionCatalog.$anonfun$getDeltaMetadata$3(DatabricksSessionCatalog.scala:169)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:759)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1698)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:760)\n\tat com.databricks.sql.DatabricksSessionCatalog.getDeltaMetadata(DatabricksSessionCatalog.scala:166)\n\tat com.databricks.sql.DatabricksSessionCatalog.getTableRawMetadata(DatabricksSessionCatalog.scala:87)\n\tat com.databricks.sql.DatabricksSessionCatalog.getTableMetadata(DatabricksSessionCatalog.scala:93)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.isDeltaTable(DeltaTable.scala:142)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableIdentifier$.apply(DeltaTableIdentifier.scala:108)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysisEdge$$anonfun$apply0$1.applyOrElse(DeltaAnalysisEdge.scala:269)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysisEdge$$anonfun$apply0$1.applyOrElse(DeltaAnalysisEdge.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:354)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:31)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysisEdge.apply0(DeltaAnalysisEdge.scala:102)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysisEdge.apply(DeltaAnalysisEdge.scala:70)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysisEdge.apply(DeltaAnalysisEdge.scala:57)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:342)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:335)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:335)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:263)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:315)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:314)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:175)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:344)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:373)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:814)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:367)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:364)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:995)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:364)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:161)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:107)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:995)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:105)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:830)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:995)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:825)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "com.databricks.sql.transaction.tahoe.DeltaTableFeatureException: Unable to read this table because it requires reader table feature(s) that are unsupported by this version of Databricks: deletionVectors. Please refer to https://docs.microsoft.com/azure/databricks/delta/feature-compatibility for more information on Delta Lake feature compatibility.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BONUS: Inspect Delta Transaction Log\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get table location\n",
    "table_location = spark.sql(f\"DESCRIBE DETAIL {TABLE_NAME}\").select(\"location\").collect()[0][0]\n",
    "print(f\"\\nTable location: {table_location}\")\n",
    "print(f\"Transaction log: {table_location}/_delta_log/\")\n",
    "\n",
    "# List log files\n",
    "print(\"\\n[LOG FILES]\")\n",
    "log_files = dbutils.fs.ls(f\"{table_location}/_delta_log/\")\n",
    "for f in sorted(log_files, key=lambda x: x.name):\n",
    "    if f.name.endswith('.json'):\n",
    "        print(f\"  {f.name}\")\n",
    "\n",
    "# Read the latest commit to see protocol\n",
    "print(\"\\n[LATEST PROTOCOL IN LOG]\")\n",
    "latest_log = sorted([f.name for f in log_files if f.name.endswith('.json')])[-1]\n",
    "log_content = spark.read.json(f\"{table_location}/_delta_log/{latest_log}\")\n",
    "\n",
    "# Show protocol information\n",
    "protocol_rows = log_content.filter(\"protocol is not null\").select(\"protocol.*\")\n",
    "if protocol_rows.count() > 0:\n",
    "    print(\"Protocol information from transaction log:\")\n",
    "    protocol_rows.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05fbbce4-a305-4668-b729-8516caf6eab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_delta_lake_properties",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}