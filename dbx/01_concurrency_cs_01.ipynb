{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3324c7de-e76f-4a07-bb83-0619374677d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test on cluster: 7c63c292\nTarget table: hive_metastore.default.concurrency_test_table\nTimestamp: 2025-10-03 11:04:55.370916\nMERGE Attempt 1: merge_7c63c292_0_1759489495\n✅ MERGE 1 successful\nMERGE Attempt 2: merge_7c63c292_1_1759489497\n✅ MERGE 2 successful\nMERGE Attempt 3: merge_7c63c292_2_1759489499\n✅ MERGE 3 successful\nMERGE Attempt 4: merge_7c63c292_3_1759489501\n✅ MERGE 4 successful\nMERGE Attempt 5: merge_7c63c292_4_1759489503\n✅ MERGE 5 successful\nMERGE Attempt 6: merge_7c63c292_5_1759489505\n✅ MERGE 6 successful\nMERGE Attempt 7: merge_7c63c292_6_1759489508\n✅ MERGE 7 successful\nMERGE Attempt 8: merge_7c63c292_7_1759489510\n✅ MERGE 8 successful\nMERGE Attempt 9: merge_7c63c292_8_1759489512\n✅ MERGE 9 successful\nMERGE Attempt 10: merge_7c63c292_9_1759489514\n✅ MERGE 10 successful\nMERGE Attempt 11: merge_7c63c292_10_1759489516\n✅ MERGE 11 successful\nMERGE Attempt 12: merge_7c63c292_11_1759489518\n✅ MERGE 12 successful\nMERGE Attempt 13: merge_7c63c292_12_1759489521\n✅ MERGE 13 successful\nMERGE Attempt 14: merge_7c63c292_13_1759489523\n✅ MERGE 14 successful\nMERGE Attempt 15: merge_7c63c292_14_1759489525\n✅ MERGE 15 successful\nMERGE Attempt 16: merge_7c63c292_15_1759489527\n✅ MERGE 16 successful\nMERGE Attempt 17: merge_7c63c292_16_1759489529\n✅ MERGE 17 successful\nMERGE Attempt 18: merge_7c63c292_17_1759489531\n✅ MERGE 18 successful\nMERGE Attempt 19: merge_7c63c292_18_1759489534\n✅ MERGE 19 successful\nMERGE Attempt 20: merge_7c63c292_19_1759489536\n✅ MERGE 20 successful\nMERGE Attempt 21: merge_7c63c292_20_1759489538\n✅ MERGE 21 successful\nMERGE Attempt 22: merge_7c63c292_21_1759489541\n✅ MERGE 22 successful\nMERGE Attempt 23: merge_7c63c292_22_1759489543\n✅ MERGE 23 successful\nMERGE Attempt 24: merge_7c63c292_23_1759489545\n✅ MERGE 24 successful\nMERGE Attempt 25: merge_7c63c292_24_1759489547\n✅ MERGE 25 successful\nMERGE Attempt 26: merge_7c63c292_25_1759489550\n✅ MERGE 26 successful\nMERGE Attempt 27: merge_7c63c292_26_1759489551\n✅ MERGE 27 successful\nMERGE Attempt 28: merge_7c63c292_27_1759489553\n✅ MERGE 28 successful\nMERGE Attempt 29: merge_7c63c292_28_1759489555\n✅ MERGE 29 successful\nMERGE Attempt 30: merge_7c63c292_29_1759489556\n✅ MERGE 30 successful\n\nMERGE test completed: 0 conflicts detected, 30 successful operations\n"
     ]
    }
   ],
   "source": [
    "# Databricks Concurrency Write Conflict Test\n",
    "# Run this test to prove write conflicts exist\n",
    "\n",
    "# Test Setup: Create this notebook and run simultaneously from 2 different clusters\n",
    "\n",
    "import time\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Configuration\n",
    "TABLE_NAME = \"hive_metastore.default.concurrency_test_table\"\n",
    "CLUSTER_ID = str(uuid.uuid4())[:8]  # Unique identifier for this cluster\n",
    "\n",
    "print(f\"Starting test on cluster: {CLUSTER_ID}\")\n",
    "print(f\"Target table: {TABLE_NAME}\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "\n",
    "# Step 2: Concurrent write test function\n",
    "def test_concurrent_writes(num_attempts=10):\n",
    "    \"\"\"\n",
    "    This function will cause write conflicts when run simultaneously\n",
    "    from multiple clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    conflicts_detected = 0\n",
    "    successful_writes = 0\n",
    "    \n",
    "    for attempt in range(num_attempts):\n",
    "        try:\n",
    "            # Generate unique operation ID\n",
    "            operation_id = f\"{CLUSTER_ID}_{attempt}_{int(time.time())}\"\n",
    "            \n",
    "            print(f\"Attempt {attempt + 1}: Starting write operation {operation_id}\")\n",
    "            \n",
    "            # Read current max ID (this creates a dependency on table state)\n",
    "            current_max = spark.sql(f\"SELECT COALESCE(MAX(id), 0) as max_id FROM {TABLE_NAME}\").collect()[0]['max_id']\n",
    "            next_id = current_max + 1\n",
    "            \n",
    "            # Small random delay to increase chance of conflict\n",
    "            time.sleep(random.uniform(0.1, 0.5))\n",
    "            \n",
    "            # Attempt to insert with next sequential ID\n",
    "            spark.sql(f\"\"\"\n",
    "            INSERT INTO {TABLE_NAME} \n",
    "            VALUES ({next_id}, '{CLUSTER_ID}', current_timestamp(), '{operation_id}')\n",
    "            \"\"\")\n",
    "            \n",
    "            successful_writes += 1\n",
    "            print(f\"✅ Write {attempt + 1} successful: ID {next_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            if \"ConcurrentAppendException\" in error_msg or \"concurrent\" in error_msg.lower():\n",
    "                conflicts_detected += 1\n",
    "                print(f\"\uD83D\uDEA8 CONFLICT DETECTED on attempt {attempt + 1}: {error_msg[:100]}...\")\n",
    "            else:\n",
    "                print(f\"❌ Other error on attempt {attempt + 1}: {error_msg[:100]}...\")\n",
    "            \n",
    "            # Brief pause before retry\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return conflicts_detected, successful_writes\n",
    "\n",
    "# Step 3: Alternative test using MERGE (more likely to cause conflicts)\n",
    "def test_merge_conflicts(num_attempts=15):\n",
    "    \"\"\"\n",
    "    MERGE operations are more likely to cause conflicts\n",
    "    \"\"\"\n",
    "    \n",
    "    conflicts_detected = 0\n",
    "    successful_operations = 0\n",
    "    \n",
    "    for attempt in range(num_attempts):\n",
    "        try:\n",
    "            operation_id = f\"merge_{CLUSTER_ID}_{attempt}_{int(time.time())}\"\n",
    "            \n",
    "            print(f\"MERGE Attempt {attempt + 1}: {operation_id}\")\n",
    "            \n",
    "            # Create a small temp view to merge\n",
    "            temp_data = spark.createDataFrame([\n",
    "                (1000 + attempt, CLUSTER_ID, datetime.now(), operation_id)\n",
    "            ], [\"id\", \"cluster_id\", \"timestamp\", \"operation_id\"])\n",
    "            \n",
    "            temp_data.createOrReplaceTempView(\"temp_merge_data\")\n",
    "            \n",
    "            # Perform MERGE operation\n",
    "            spark.sql(f\"\"\"\n",
    "            MERGE INTO {TABLE_NAME} AS target\n",
    "            USING temp_merge_data AS source\n",
    "            ON target.id = source.id\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET \n",
    "                    target.cluster_id = source.cluster_id,\n",
    "                    target.timestamp = source.timestamp,\n",
    "                    target.operation_id = source.operation_id\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT (id, cluster_id, timestamp, operation_id)\n",
    "                VALUES (source.id, source.cluster_id, source.timestamp, source.operation_id)\n",
    "            \"\"\")\n",
    "            \n",
    "            successful_operations += 1\n",
    "            print(f\"✅ MERGE {attempt + 1} successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            if \"ConcurrentAppendException\" in error_msg or \"concurrent\" in error_msg.lower():\n",
    "                conflicts_detected += 1\n",
    "                print(f\"\uD83D\uDEA8 MERGE CONFLICT DETECTED on attempt {attempt + 1}: {error_msg[:100]}...\")\n",
    "            else:\n",
    "                print(f\"❌ MERGE error on attempt {attempt + 1}: {error_msg[:100]}...\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "    return conflicts_detected, successful_operations\n",
    "\n",
    "# Step 4: Results analysis\n",
    "def analyze_results():\n",
    "    \"\"\"Check the final state of the table\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL RESULTS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Count total rows\n",
    "    total_rows = spark.sql(f\"SELECT COUNT(*) as count FROM {TABLE_NAME}\").collect()[0]['count']\n",
    "    print(f\"Total rows in table: {total_rows}\")\n",
    "    \n",
    "    # Show distribution by cluster\n",
    "    cluster_distribution = spark.sql(f\"\"\"\n",
    "    SELECT cluster_id, COUNT(*) as row_count \n",
    "    FROM {TABLE_NAME} \n",
    "    GROUP BY cluster_id \n",
    "    ORDER BY cluster_id\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    print(\"\\nRows by cluster:\")\n",
    "    for row in cluster_distribution:\n",
    "        print(f\"  {row['cluster_id']}: {row['row_count']} rows\")\n",
    "    \n",
    "    # Show recent operations\n",
    "    print(f\"\\nRecent operations from cluster {CLUSTER_ID}:\")\n",
    "    recent_ops = spark.sql(f\"\"\"\n",
    "    SELECT * FROM {TABLE_NAME} \n",
    "    WHERE cluster_id = '{CLUSTER_ID}' \n",
    "    ORDER BY timestamp DESC \n",
    "    LIMIT 5\n",
    "    \"\"\").show()\n",
    "\n",
    "# Step 2: Concurrent test (run simultaneously from 2 clusters)\n",
    "# conflicts, successes = test_concurrent_writes(20)\n",
    "# print(f\"\\nTest completed: {conflicts} conflicts detected, {successes} successful writes\")\n",
    "\n",
    "print(f\"Cluster {CLUSTER_ID} waiting to start at synchronized time...\")\n",
    "time.sleep(10)  # Give time to start both notebooks\n",
    "print(\"Starting NOW!\")\n",
    "\n",
    "# Step 3: MERGE test (run simultaneously from 2 clusters) \n",
    "merge_conflicts, merge_successes = test_merge_conflicts(30)\n",
    "print(f\"\\nMERGE test completed: {merge_conflicts} conflicts detected, {merge_successes} successful operations\")\n",
    "\n",
    "# Step 4: Results\n",
    "#analyze_results()\n",
    "\n",
    "#print(f\"\\nCluster {CLUSTER_ID} ready for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d85912-5aaf-4602-931e-b53b90b5bfef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_concurrency_cs_01",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}